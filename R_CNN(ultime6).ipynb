{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/SAeoHj0y/bJFiBKHnL1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easytoday/RCNN-region-CNN/blob/main/R_CNN(ultime6).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxVBloJAWQUY",
        "outputId": "b1dad6a3-9e3a-460a-9451-6c8032ca7ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installation de pycocotools ---\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (3.0.12)\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-nsuciz2h\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-nsuciz2h\n",
            "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools==2.0) (75.2.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.11/dist-packages (from pycocotools==2.0) (3.0.12)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools==2.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0) (1.17.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp311-cp311-linux_x86_64.whl size=395986 sha256=12748e9d12d12c4ef9e3acc3ae24aefcf6fb37a07085acc36c403dd719f4a0c1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nexwfxsc/wheels/6d/69/75/358c50a37672dfda8d74ba3b30ec49fb75d52f7c081886d503\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.9\n",
            "    Uninstalling pycocotools-2.0.9:\n",
            "      Successfully uninstalled pycocotools-2.0.9\n",
            "Successfully installed pycocotools-2.0\n",
            "pycocotools installé avec succès.\n",
            "\n",
            "--- Téléchargement des images de validation COCO 2017 dans /content/coco_dataset/images/val2017 ---\n",
            "--2025-06-15 22:12:32--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.160.161, 54.231.233.49, 52.217.32.124, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.160.161|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘/content/coco_dataset/val2017.zip’\n",
            "\n",
            "val2017.zip         100%[===================>] 777.80M  18.3MB/s    in 47s     \n",
            "\n",
            "2025-06-15 22:13:19 (16.6 MB/s) - ‘/content/coco_dataset/val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "--- Téléchargement des annotations COCO 2017 dans /content/coco_dataset/annotations ---\n",
            "--2025-06-15 22:13:19--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.13.54, 52.217.100.196, 3.5.29.76, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.13.54|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘/content/coco_dataset/annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  18.2MB/s    in 15s     \n",
            "\n",
            "2025-06-15 22:13:35 (15.9 MB/s) - ‘/content/coco_dataset/annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n",
            "\n",
            "--- Décompression des fichiers COCO ---\n",
            "val2017.zip décompressé.\n",
            "annotations_trainval2017.zip décompressé.\n",
            "\n",
            "--- Téléchargement et décompression COCO terminés. ---\n"
          ]
        }
      ],
      "source": [
        "# --- INSTALLATION DE PYCOCOTOOLS ---\n",
        "print(\"--- Installation de pycocotools ---\")\n",
        "!pip install cython\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "\n",
        "# Test de l'installation\n",
        "try:\n",
        "    from pycocotools.coco import COCO\n",
        "    print(\"pycocotools installé avec succès.\")\n",
        "except ImportError:\n",
        "    print(\"ERREUR: pycocotools n'a pas pu être importé. Vérifiez l'installation.\")\n",
        "    import sys; sys.exit(\"Impossible de continuer sans pycocotools.\")\n",
        "\n",
        "\n",
        "# --- TÉLÉCHARGEMENT DE MS COCO VAL2017 ---\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "COCO_DATA_DIR = '/content/coco_dataset'\n",
        "os.makedirs(COCO_DATA_DIR, exist_ok=True)\n",
        "\n",
        "COCO_VAL_IMAGES_URL = 'http://images.cocodataset.org/zips/val2017.zip'\n",
        "COCO_VAL_ANNOTATIONS_URL = 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
        "\n",
        "print(f\"\\n--- Téléchargement des images de validation COCO 2017 dans {COCO_DATA_DIR}/images/val2017 ---\")\n",
        "!wget -c {COCO_VAL_IMAGES_URL} -P {COCO_DATA_DIR}\n",
        "print(f\"--- Téléchargement des annotations COCO 2017 dans {COCO_DATA_DIR}/annotations ---\")\n",
        "!wget -c {COCO_VAL_ANNOTATIONS_URL} -P {COCO_DATA_DIR}\n",
        "\n",
        "print(\"\\n--- Décompression des fichiers COCO ---\")\n",
        "with zipfile.ZipFile(os.path.join(COCO_DATA_DIR, 'val2017.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(os.path.join(COCO_DATA_DIR, 'images'))\n",
        "    print(\"val2017.zip décompressé.\")\n",
        "\n",
        "with zipfile.ZipFile(os.path.join(COCO_DATA_DIR, 'annotations_trainval2017.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(COCO_DATA_DIR)\n",
        "    print(\"annotations_trainval2017.zip décompressé.\")\n",
        "\n",
        "print(\"\\n--- Téléchargement et décompression COCO terminés. ---\")\n",
        "\n",
        "# chemins pour qu'ils soient accessibles\n",
        "COCO_IMAGES_VAL_PATH = os.path.join(COCO_DATA_DIR, 'images', 'val2017')\n",
        "COCO_ANNOTATIONS_VAL_PATH = os.path.join(COCO_DATA_DIR, 'annotations', 'instances_val2017.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler # Ajouté pour la normalisation SVM\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import time\n",
        "import json\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "# --- 0. Configuration Initiale et Montage de Google Drive ---\n",
        "print(\"--- Initialisation du pipeline R-CNN ---\")\n",
        "\n",
        "start_time_global = datetime.now()\n",
        "timestamp = start_time_global.strftime('%Y%m%d_%H%M%S')\n",
        "print(f\"Heure de début de l'exécution : {start_time_global.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "report_content_list = []\n",
        "\n",
        "REPORT_FOLDER_DRIVE = '/content/drive/MyDrive/RCP209/Rapports_RCNN'\n",
        "REPORT_FOLDER_LOCAL = './rapports_locaux'\n",
        "os.makedirs(REPORT_FOLDER_LOCAL, exist_ok=True)\n",
        "log_report_local_path = os.path.join(REPORT_FOLDER_LOCAL, f'rapport_rcnn_{timestamp}_local_backup.md')\n",
        "report_file_path = None\n",
        "\n",
        "def log_report(message, level=\"INFO\", to_console=True):\n",
        "    current_time = datetime.now().strftime('[%H:%M:%S]')\n",
        "    log_line = f\"[{level}] {current_time} {message}\\n\"\n",
        "    report_content_list.append(log_line)\n",
        "    if to_console:\n",
        "        print(log_line.strip())\n",
        "\n",
        "def save_report_to_file():\n",
        "    global report_file_path, report_content_list, log_report_local_path\n",
        "    if report_file_path is None:\n",
        "        report_file_path = log_report_local_path\n",
        "    try:\n",
        "        with open(report_file_path, 'w', encoding='utf-8') as f:\n",
        "            f.writelines(report_content_list)\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            with open(log_report_local_path, 'w', encoding='utf-8') as f:\n",
        "                f.writelines(report_content_list)\n",
        "        except Exception as e_local:\n",
        "            pass\n",
        "\n",
        "log_report(\"Tentative de montage de Google Drive...\", level=\"SETUP\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.makedirs(REPORT_FOLDER_DRIVE, exist_ok=True)\n",
        "    report_file_path = os.path.join(REPORT_FOLDER_DRIVE, f'rapport_rcnn_{timestamp}.md')\n",
        "    log_report(f\"Google Drive monté avec succès. Les rapports seront enregistrés dans : {REPORT_FOLDER_DRIVE}\", level=\"SETUP\")\n",
        "except Exception as e:\n",
        "    log_report(f\"ERREUR: Impossible de monter Google Drive ou de créer le dossier de rapports : {e}\", level=\"ERROR\")\n",
        "    log_report(f\"Les rapports seront uniquement sauvegardés localement dans : {REPORT_FOLDER_LOCAL}\", level=\"WARNING\")\n",
        "    report_file_path = log_report_local_path\n",
        "\n",
        "report_content_list.append(f\"# Rapport d'Exécution R-CNN - {timestamp}\\n\\n\")\n",
        "report_content_list.append(f\"## Résumé de l'Expérience\\n\")\n",
        "report_content_list.append(f\"* Date et Heure de début de l'exécution : {start_time_global.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "log_report(\"\\n--- Vérification de l'environnement ---\", to_console=True)\n",
        "report_content_list.append(\"\\n---\\n\\n## Configuration de l'Environnement\\n\")\n",
        "\n",
        "log_report(f\"PyTorch version: {torch.__version__}\", level=\"INFO\")\n",
        "report_content_list.append(f\"* Version PyTorch : `{torch.__version__}`\\n\")\n",
        "log_report(f\"Torchvision version: {torchvision.__version__}\", level=\"INFO\")\n",
        "report_content_list.append(f\"* Version Torchvision : `{torchvision.__version__}`\\n\")\n",
        "log_report(f\"OpenCV version: {cv2.__version__}\", level=\"INFO\")\n",
        "report_content_list.append(f\"* Version OpenCV : `{cv2.__version__}`\\n\")\n",
        "log_report(f\"NumPy version: {np.__version__}\", level=\"INFO\")\n",
        "report_content_list.append(f\"* Version NumPy : `{np.__version__}`\\n\")\n",
        "\n",
        "from sklearn import __version__ as sklearn_version\n",
        "log_report(f\"Scikit-learn version: {sklearn_version}\", level=\"INFO\")\n",
        "report_content_list.append(f\"* Version Scikit-learn : `{sklearn_version}`\\n\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    device_properties = torch.cuda.get_device_properties(0)\n",
        "    total_memory_gb = device_properties.total_memory / (1024**3)\n",
        "    log_report(f\"Un GPU est disponible : {device_name} avec {total_memory_gb:.2f} GB de mémoire.\", level=\"INFO\")\n",
        "    report_content_list.append(f\"* GPU Utilisé : `{device_name}`\\n\")\n",
        "    report_content_list.append(f\"  * Mémoire totale GPU : `{total_memory_gb:.2f} GB`\\n\")\n",
        "    report_content_list.append(\"Le GPU sera utilisé automatiquement pour les calculs intensifs.\\n\")\n",
        "else:\n",
        "    log_report(\"Aucun GPU trouvé. Toutes les opérations s'exécuteront sur CPU (plus lent voire impossible).\", level=\"WARNING\")\n",
        "    report_content_list.append(\"* GPU Utilisé : `N/A (CPU seulement)`\\n\")\n",
        "    report_content_list.append(\"Vérifiez votre type de runtime (Exécution > Changer le type d'exécution).\\n\")\n",
        "\n",
        "# --- DÉFINITIONS GLOBALES DE CLASSES ET MAPPINGS ---\n",
        "PASCAL_VOC_CLASSES = [\n",
        "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
        "    \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "]\n",
        "NUM_CLASSES_VOC = len(PASCAL_VOC_CLASSES)\n",
        "CNN_CLASSES = PASCAL_VOC_CLASSES + [\"background\"]\n",
        "\n",
        "COCO_TO_VOC_CLASSES = {\n",
        "    'person': 'person', 'bicycle': 'bicycle', 'car': 'car', 'motorcycle': 'motorbike', 'airplane': 'aeroplane',\n",
        "    'bus': 'bus', 'train': 'train', 'truck': 'car',\n",
        "    'boat': 'boat', 'traffic light': None, 'fire hydrant': None,\n",
        "    'stop sign': None, 'parking meter': None, 'bench': None, 'bird': 'bird', 'cat': 'cat', 'dog': 'dog',\n",
        "    'horse': 'horse', 'sheep': 'sheep', 'cow': 'cow', 'elephant': None, 'bear': None, 'zebra': None,\n",
        "    'giraffe': None, 'backpack': None, 'umbrella': None, 'handbag': None, 'tie': None, 'suitcase': None,\n",
        "    'frisbee': None, 'skis': None, 'snowboard': None, 'sports ball': None, 'kite': None, 'baseball bat': None,\n",
        "    'baseball glove': None, 'skateboard': None, 'surfboard': None, 'tennis racket': None, 'bottle': 'bottle',\n",
        "    'wine glass': None, 'cup': None, 'fork': None, 'knife': None, 'spoon': None, 'bowl': None, 'banana': None,\n",
        "    'apple': None, 'sandwich': None, 'orange': None, 'broccoli': None, 'carrot': None, 'hot dog': None,\n",
        "    'pizza': None, 'donut': None, 'cake': None, 'chair': 'chair', 'couch': 'sofa',\n",
        "    'potted plant': 'pottedplant', 'bed': None, 'dining table': 'diningtable', 'toilet': None, 'tv': 'tvmonitor',\n",
        "    'laptop': None, 'mouse': None, 'remote': None, 'keyboard': None, 'cell phone': None, 'microwave': None,\n",
        "    'oven': None, 'toaster': None, 'sink': None, 'refrigerator': None, 'book': None, 'clock': None,\n",
        "    'vase': None, 'scissors': None, 'teddy bear': None, 'hair drier': None, 'toothbrush': None\n",
        "}\n",
        "\n",
        "# --- DÉFINITION DES CLASSES DATASET ---\n",
        "class PascalVOCDataset(Dataset):\n",
        "    def __init__(self, root_dir, year='2007', image_set='trainval', transform=None):\n",
        "        self.voc_path = root_dir\n",
        "        self.year = year\n",
        "        self.image_set = image_set\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_dir = os.path.join(self.voc_path, 'JPEGImages')\n",
        "        self.annotation_dir = os.path.join(self.voc_path, 'Annotations')\n",
        "        self.image_set_path = os.path.join(self.voc_path, 'ImageSets', 'Main', image_set + '.txt')\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.annotations_data = {}\n",
        "\n",
        "        log_report(f\"Préparation du chargement du dataset PASCAL VOC {year} {image_set} depuis {self.voc_path}...\", level=\"INFO\")\n",
        "\n",
        "        if not os.path.exists(self.image_set_path):\n",
        "            log_report(f\"ATTENTION : Fichier ImageSet non trouvé à {self.image_set_path}.\", level=\"ERROR\")\n",
        "            sys.exit(\"Fichier ImageSet critique manquant. Arrêt du script.\")\n",
        "\n",
        "        with open(self.image_set_path, 'r') as f:\n",
        "            image_ids = [line.strip() for line in f]\n",
        "\n",
        "        for img_id in image_ids:\n",
        "            img_path = os.path.join(self.image_dir, f'{img_id}.jpg')\n",
        "            ann_path = os.path.join(self.annotation_dir, f'{img_id}.xml')\n",
        "\n",
        "            if os.path.exists(img_path) and os.path.exists(ann_path):\n",
        "                objs = self._parse_annotation_file(ann_path)\n",
        "                if objs:\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.annotations_data[img_path] = objs\n",
        "\n",
        "        log_report(f\"Structure du Dataset PascalVOCDataset initialisée. Prêt à charger {len(self.image_paths)} images.\", level=\"INFO\")\n",
        "\n",
        "    def _parse_annotation_file(self, xml_file_path):\n",
        "        tree = ET.parse(xml_file_path)\n",
        "        root = tree.getroot()\n",
        "        objects = []\n",
        "        for obj in root.findall('object'):\n",
        "            class_name = obj.find('name').text\n",
        "            if class_name not in PASCAL_VOC_CLASSES:\n",
        "                continue\n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin = float(bbox.find('xmin').text) - 1\n",
        "            ymin = float(bbox.find('ymin').text) - 1\n",
        "            xmax = float(bbox.find('xmax').text) - 1\n",
        "            ymax = float(bbox.find('ymax').text) - 1\n",
        "\n",
        "            xmin = max(0.0, xmin)\n",
        "            ymin = max(0.0, ymin)\n",
        "            xmax = max(xmin + 1, xmax)\n",
        "            ymax = max(ymin + 1, ymax)\n",
        "\n",
        "            objects.append({\n",
        "                'class_name': class_name,\n",
        "                'bbox': [xmin, ymin, xmax, ymax]\n",
        "            })\n",
        "        return objects\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        annotations = self.annotations_data.get(img_path, [])\n",
        "\n",
        "        # Le transform sera appliqué sur les régions pour le fine-tuning\n",
        "        # Mais ici on renvoie l'image PIL pour selective search\n",
        "        # Pour un transform sur l'image entière faire:\n",
        "        # if self.transform:\n",
        "        #     image = self.transform(image)\n",
        "\n",
        "        return image, annotations, img_path\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, root_dir, annotation_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.ids = list(self.coco.imgs.keys())\n",
        "\n",
        "        self.coco_cats = self.coco.loadCats(self.coco.getCatIds())\n",
        "        self.coco_id_to_name = {cat['id']: cat['name'] for cat in self.coco_cats}\n",
        "        self.voc_class_to_idx = {cls_name: i for i, cls_name in enumerate(PASCAL_VOC_CLASSES)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.ids[index]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=False)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann['bbox']\n",
        "            bbox = [x, y, x + w, y + h]\n",
        "\n",
        "            coco_cat_name = self.coco_id_to_name[ann['category_id']]\n",
        "            voc_class_name = COCO_TO_VOC_CLASSES.get(coco_cat_name)\n",
        "\n",
        "            if voc_class_name and voc_class_name in PASCAL_VOC_CLASSES:\n",
        "                boxes.append(bbox)\n",
        "                labels.append(self.voc_class_to_idx[voc_class_name])\n",
        "\n",
        "        if len(boxes) > 0:\n",
        "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.tensor(labels, dtype=torch.long)\n",
        "        else:\n",
        "            boxes = torch.empty((0, 4), dtype=torch.float32)\n",
        "            labels = torch.empty((0,), dtype=torch.long)\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor([img_id])\n",
        "        }\n",
        "\n",
        "        # Le transform sera appliqué plus tard sur les régions d'intérêt, pas sur l'image complète ici\n",
        "        # Pour le faire sur l'image compléte faire:\n",
        "        # if self.transform:\n",
        "        #     image = self.transform(image)\n",
        "\n",
        "        return image, target, img_path\n",
        "\n",
        "# --- Fonctions utilitaires ---\n",
        "def custom_collate_fn(batch):\n",
        "    if len(batch) == 1:\n",
        "        return batch[0][0], batch[0][1], batch[0][2]\n",
        "    else:\n",
        "        raise ValueError(\"custom_collate_fn attend un batch de taille 1 pour cette configuration.\")\n",
        "\n",
        "def get_selective_search_proposals(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        log_report(f\"Avertissement: Impossible de lire l'image {image_path}. Retourne des propositions vides.\", level=\"WARNING\")\n",
        "        return []\n",
        "    try:\n",
        "        ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
        "    except AttributeError:\n",
        "        log_report(\"Erreur : cv2.ximgproc.segmentation.createSelectiveSearchSegmentation n'est pas disponible.\", level=\"ERROR\")\n",
        "        sys.exit(\"Module OpenCV ximgproc manquant. Arrêt du script.\")\n",
        "    ss.setBaseImage(img)\n",
        "    ss.switchToSelectiveSearchFast()\n",
        "    rects = ss.process()\n",
        "    proposals_xyxy = []\n",
        "    for (x, y, w, h) in rects:\n",
        "        if w > 0 and h > 0:\n",
        "            proposals_xyxy.append([x, y, x + w, y + h])\n",
        "    return proposals_xyxy[:2000]\n",
        "\n",
        "def calculate_iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    inter_width = max(0, xB - xA)\n",
        "    inter_height = max(0, yB - yA)\n",
        "    inter_area = inter_width * inter_height\n",
        "    boxA_area = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxB_area = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "    union_area = float(boxA_area + boxB_area - inter_area)\n",
        "    if union_area == 0:\n",
        "        return 0.0\n",
        "    return inter_area / union_area\n",
        "\n",
        "def warp_region(image_pil, bbox, target_size=(227, 227), context_pad=16):\n",
        "    x_min, y_min, x_max, y_max = bbox\n",
        "    img_width, img_height = image_pil.size\n",
        "\n",
        "    x_min_padded = max(0, int(x_min - context_pad))\n",
        "    y_min_padded = max(0, int(y_min - context_pad))\n",
        "    x_max_padded = min(img_width, int(x_max + context_pad))\n",
        "    y_max_padded = min(img_height, int(y_max + context_pad))\n",
        "\n",
        "    if x_max_padded <= x_min_padded or y_max_padded <= y_min_padded:\n",
        "        return Image.new('RGB', target_size, (0, 0, 0))\n",
        "\n",
        "    region = image_pil.crop((x_min_padded, y_min_padded, x_max_padded, y_max_padded))\n",
        "    warped_region = region.resize(target_size, Image.BILINEAR)\n",
        "    return warped_region\n",
        "\n",
        "# --- Fonctions CNN / SVM / Détection ---\n",
        "def fine_tune_cnn(cnn_model, train_dataset, num_epochs=5, lr=0.001, batch_size=32, iou_threshold=0.5, image_transform=None):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    cnn_model.to(device)\n",
        "    cnn_model.train()\n",
        "\n",
        "    num_ftrs = cnn_model.classifier[6].in_features\n",
        "    cnn_model.classifier[6] = nn.Linear(num_ftrs, NUM_CLASSES_VOC + 1).to(device)\n",
        "\n",
        "    optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: x[0])\n",
        "\n",
        "    log_report(f\"Démarrage du fine-tuning du CNN pour {num_epochs} époques...\", level=\"INFO\")\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        for i, (image_pil, annotations, img_path) in enumerate(data_loader):\n",
        "            if not isinstance(image_pil, Image.Image): # Si le dataset applique déjà le transform\n",
        "                # reconvertir si c'est un tenseur\n",
        "                if isinstance(image_pil, torch.Tensor):\n",
        "                    image_pil = transforms.ToPILImage()(image_pil.cpu())\n",
        "                else:\n",
        "                    log_report(f\"Type d'image inattendu: {type(image_pil)}. Skipping image.\", level=\"WARNING\")\n",
        "                    continue\n",
        "\n",
        "            proposals = get_selective_search_proposals(img_path)\n",
        "            if not proposals:\n",
        "                continue\n",
        "\n",
        "            positive_regions = []\n",
        "            negative_regions = []\n",
        "\n",
        "            for prop_bbox in proposals:\n",
        "                max_iou = 0.0\n",
        "                best_gt_class_idx = -1\n",
        "\n",
        "                for gt_obj in annotations:\n",
        "                    iou = calculate_iou(prop_bbox, gt_obj['bbox'])\n",
        "                    if iou > max_iou:\n",
        "                        max_iou = iou\n",
        "                        if gt_obj['class_name'] in PASCAL_VOC_CLASSES:\n",
        "                            best_gt_class_idx = PASCAL_VOC_CLASSES.index(gt_obj['class_name'])\n",
        "\n",
        "                if max_iou >= iou_threshold and best_gt_class_idx != -1:\n",
        "                    positive_regions.append((prop_bbox, best_gt_class_idx))\n",
        "                elif max_iou < 0.2:\n",
        "                    negative_regions.append(prop_bbox)\n",
        "\n",
        "            regions_for_cnn = []\n",
        "            labels_for_cnn = []\n",
        "\n",
        "            for bbox, class_idx in positive_regions:\n",
        "                regions_for_cnn.append(warp_region(image_pil, bbox))\n",
        "                labels_for_cnn.append(class_idx)\n",
        "\n",
        "            num_negative_to_add = min(len(negative_regions), len(positive_regions) * 3 if positive_regions else 50)\n",
        "            if negative_regions:\n",
        "                random.shuffle(negative_regions)\n",
        "                for bbox in negative_regions[:num_negative_to_add]:\n",
        "                    regions_for_cnn.append(warp_region(image_pil, bbox))\n",
        "                    labels_for_cnn.append(NUM_CLASSES_VOC)\n",
        "\n",
        "            if not regions_for_cnn:\n",
        "                continue\n",
        "\n",
        "            processed_regions = [image_transform(region).to(device) for region in regions_for_cnn]\n",
        "            labels = torch.tensor(labels_for_cnn, dtype=torch.long).to(device)\n",
        "\n",
        "            if not processed_regions:\n",
        "                continue\n",
        "            batch_regions = torch.stack(processed_regions)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn_model(batch_regions)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * batch_regions.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_loss = epoch_loss / total_samples if total_samples > 0 else 0\n",
        "        accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
        "        log_report(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\", level=\"INFO\")\n",
        "\n",
        "    log_report(\"Fine-tuning du CNN terminé.\", level=\"INFO\")\n",
        "    return cnn_model\n",
        "\n",
        "def extract_features(cnn_model, image_pil, proposals, image_transform):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    cnn_model.to(device)\n",
        "    cnn_model.eval()\n",
        "\n",
        "    features_list = []\n",
        "\n",
        "    if not proposals:\n",
        "        return np.array([])\n",
        "\n",
        "    processed_regions = [warp_region(image_pil, bbox) for bbox in proposals]\n",
        "    transformed_regions = [image_transform(region).to(device) for region in processed_regions]\n",
        "\n",
        "    if not transformed_regions:\n",
        "        return np.array([])\n",
        "\n",
        "    regions_batch = torch.stack(transformed_regions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x = cnn_model.features(regions_batch)\n",
        "        x = cnn_model.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        for i, layer in enumerate(cnn_model.classifier):\n",
        "            x = layer(x)\n",
        "            if i == 5:\n",
        "                break\n",
        "\n",
        "        features_list.append(x.cpu().numpy())\n",
        "\n",
        "    if features_list:\n",
        "        return np.vstack(features_list)\n",
        "    return np.array([])\n",
        "\n",
        "def train_svms(train_dataset, fine_tuned_cnn_model, iou_threshold=0.5, image_transform=None):   #etait à 0.5 on change à 0.3 revient à 0.5\n",
        "    log_report(\"Démarrage de l'entraînement des SVMs...\", level=\"INFO\")\n",
        "    trained_svms = {}\n",
        "\n",
        "    all_class_features = defaultdict(lambda: {'features': [], 'labels': []})\n",
        "\n",
        "    data_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: x[0])\n",
        "\n",
        "    for i, (image_pil, annotations, img_path) in enumerate(data_loader):\n",
        "        if not isinstance(image_pil, Image.Image): # Si le dataset applique déjà le transform\n",
        "            if isinstance(image_pil, torch.Tensor):\n",
        "                image_pil = transforms.ToPILImage()(image_pil.cpu())\n",
        "            else:\n",
        "                log_report(f\"Type d'image inattendu: {type(image_pil)}. Skipping image.\", level=\"WARNING\")\n",
        "                continue\n",
        "\n",
        "        proposals = get_selective_search_proposals(img_path)\n",
        "\n",
        "        if not proposals:\n",
        "            continue\n",
        "\n",
        "        all_proposal_features = extract_features(fine_tuned_cnn_model, image_pil, proposals, image_transform)\n",
        "        if all_proposal_features.size == 0:\n",
        "            continue\n",
        "\n",
        "        if len(proposals) != len(all_proposal_features):\n",
        "            log_report(f\"Disparité propositions/features pour {img_path}. Ignoré.\", level=\"WARNING\")\n",
        "            continue\n",
        "\n",
        "        for idx, prop_bbox in enumerate(proposals):\n",
        "            feature = all_proposal_features[idx]\n",
        "            max_iou = 0.0\n",
        "            best_gt_class_name = None\n",
        "\n",
        "            for gt_obj in annotations:\n",
        "                iou = calculate_iou(prop_bbox, gt_obj['bbox'])\n",
        "                if iou > max_iou:\n",
        "                    max_iou = iou\n",
        "                    best_gt_class_name = gt_obj['class_name']\n",
        "\n",
        "            if max_iou >= iou_threshold and best_gt_class_name in PASCAL_VOC_CLASSES:\n",
        "                for class_name in PASCAL_VOC_CLASSES:\n",
        "                    if class_name == best_gt_class_name:\n",
        "                        all_class_features[class_name]['features'].append(feature)\n",
        "                        all_class_features[class_name]['labels'].append(1)\n",
        "                    else:\n",
        "                        all_class_features[class_name]['features'].append(feature)\n",
        "                        all_class_features[class_name]['labels'].append(0)\n",
        "\n",
        "            elif max_iou < 0.2:\n",
        "                for class_name in PASCAL_VOC_CLASSES:\n",
        "                    all_class_features[class_name]['features'].append(feature)\n",
        "                    all_class_features[class_name]['labels'].append(0)\n",
        "\n",
        "    for class_name in PASCAL_VOC_CLASSES:\n",
        "        log_report(f\"Entraînement du SVM pour la classe '{class_name}'...\", level=\"INFO\")\n",
        "        #features = np.array(all_class_features[class_name]['features'])\n",
        "        #labels = np.array(all_class_features[class_name]['labels'])\n",
        "\n",
        "        # S'assurer qu'il y a des échantillons positifs et négatifs\n",
        "        #if len(features) == 0 or np.sum(labels) == 0 or np.sum(labels == 0) == 0:\n",
        "        #    log_report(f\"Pas assez d'échantillons positifs/négatifs pour entraîner le SVM pour '{class_name}'. Skip.\", level=\"WARNING\")\n",
        "        #    continue\n",
        "\n",
        "        positive_features_list = [f for f, l in zip(all_class_features[class_name]['features'], all_class_features[class_name]['labels']) if l == 1]\n",
        "        negative_features_list = [f for f, l in zip(all_class_features[class_name]['features'], all_class_features[class_name]['labels']) if l == 0]\n",
        "\n",
        "        # Convertir les listes en tableaux numpy\n",
        "        positive_features = np.array(positive_features_list) if positive_features_list else np.empty((0, all_class_features[class_name]['features'][0].shape[0]))\n",
        "        negative_features = np.array(negative_features_list) if negative_features_list else np.empty((0, all_class_features[class_name]['features'][0].shape[0]))\n",
        "\n",
        "        # --- Logique d'échantillonnage des négatives ---\n",
        "        num_pos = len(positive_features)\n",
        "        num_neg = len(negative_features)\n",
        "\n",
        "        # ratio ou un nombre maximum de négatifs à inclure\n",
        "        # -> Une bonne pratique est de limiter les négatives à 3 fois le nombre de positives,\n",
        "        # ou à un maximum absolu pour les très grands datasets.\n",
        "        MAX_NEGATIVES_RATIO = 10 # 5 puis 10 et maintenat 15s\n",
        "        MAX_NEGATIVES_ABSOLUTE = 100000 # Vous pouvez ajuster cette valeur (ex: 50000, 200000)\n",
        "\n",
        "        negative_features_sampled = np.empty((0, positive_features.shape[1])) # Initialisation avec la bonne dimension\n",
        "\n",
        "        if num_neg > 0:\n",
        "            # Calculer le nombre cible de négatives à inclure\n",
        "            target_negatives_count = min(num_neg, int(num_pos * MAX_NEGATIVES_RATIO), MAX_NEGATIVES_ABSOLUTE)\n",
        "\n",
        "            if num_neg > target_negatives_count:\n",
        "                # Échantillonnage aléatoire des indices\n",
        "                neg_indices = np.random.choice(num_neg, target_negatives_count, replace=False)\n",
        "                negative_features_sampled = negative_features[neg_indices]\n",
        "                log_report(f\"Échantillonnage de {len(negative_features_sampled)} négatifs sur {num_neg} pour la classe '{class_name}'.\", level=\"INFO\")\n",
        "            else:\n",
        "                negative_features_sampled = negative_features\n",
        "        else:\n",
        "            log_report(f\"Aucun échantillon négatif pour la classe '{class_name}'.\", level=\"WARNING\")\n",
        "\n",
        "\n",
        "        # Combiner les échantillons positifs et les négatifs échantillonnés\n",
        "        features = np.vstack((positive_features, negative_features_sampled))\n",
        "        labels = np.hstack((np.ones(len(positive_features)), np.zeros(len(negative_features_sampled))))\n",
        "\n",
        "        # S'assurer qu'il y a des échantillons pour l'entraînement du SVM après échantillonnage\n",
        "        if len(features) == 0 or np.sum(labels) == 0 or np.sum(labels == 0) == 0:\n",
        "            log_report(f\"Pas assez d'échantillons positifs/négatifs pour entraîner le SVM pour '{class_name}' après échantillonnage. Skip.\", level=\"WARNING\")\n",
        "            continue\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "        svm = LinearSVC(C=0.0005, random_state=42, max_iter=2000) # C was 0.001 puis 0.0001 et maintenant 0.01\n",
        "        svm.fit(features_scaled, labels)\n",
        "        trained_svms[class_name] = {'svm': svm, 'scaler': scaler}\n",
        "        log_report(f\"SVM pour '{class_name}' entraîné. Score de classification (accuracy): {svm.score(features_scaled, labels):.4f}\", level=\"INFO\")\n",
        "\n",
        "    log_report(\"Entraînement des SVMs terminé.\", level=\"INFO\")\n",
        "    return trained_svms\n",
        "\n",
        "def detect_objects(image_path, fine_tuned_cnn_model, trained_svms, iou_nms_threshold=0.3, image_transform=None):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    fine_tuned_cnn_model.to(device)\n",
        "    fine_tuned_cnn_model.eval()\n",
        "\n",
        "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "    proposals = get_selective_search_proposals(image_path)\n",
        "\n",
        "    if not proposals:\n",
        "        return []\n",
        "\n",
        "    all_proposal_features = extract_features(fine_tuned_cnn_model, image_pil, proposals, image_transform)\n",
        "\n",
        "    if all_proposal_features.size == 0:\n",
        "        return []\n",
        "\n",
        "    detections_by_class = defaultdict(list)\n",
        "\n",
        "    for class_name in PASCAL_VOC_CLASSES:\n",
        "        if class_name not in trained_svms:\n",
        "            continue\n",
        "\n",
        "        svm_data = trained_svms[class_name]\n",
        "        svm = svm_data['svm']\n",
        "        scaler = svm_data['scaler']\n",
        "\n",
        "        features_scaled = scaler.transform(all_proposal_features)\n",
        "        scores = svm.decision_function(features_scaled)\n",
        "\n",
        "        for i, score in enumerate(scores):\n",
        "            bbox = proposals[i]\n",
        "            detections_by_class[class_name].append((score, bbox))\n",
        "\n",
        "    final_detections = []\n",
        "\n",
        "    for class_name, detections in detections_by_class.items():\n",
        "        if not detections:\n",
        "            continue\n",
        "\n",
        "        detections.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        boxes = [d[1] for d in detections]\n",
        "        scores = [d[0] for d in detections]\n",
        "\n",
        "        boxes_tensor = torch.tensor(boxes, dtype=torch.float32).to(device)\n",
        "        scores_tensor = torch.tensor(scores, dtype=torch.float32).to(device)\n",
        "\n",
        "        indices = torchvision.ops.nms(boxes_tensor, scores_tensor, iou_nms_threshold)\n",
        "\n",
        "        for idx in indices:\n",
        "            score = scores[idx.item()]\n",
        "            bbox = boxes[idx.item()]\n",
        "            final_detections.append((class_name, score, bbox))\n",
        "\n",
        "    final_detections.sort(key=lambda x: x[1], reverse=True)\n",
        "    return final_detections\n",
        "\n",
        "def evaluate_detector(test_dataset, fine_tuned_cnn_model, trained_svms, iou_threshold_for_tp=0.5, iou_nms_threshold=0.3, image_transform=None):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    fine_tuned_cnn_model.to(device)\n",
        "    fine_tuned_cnn_model.eval()\n",
        "\n",
        "    all_gt_counts_by_class = defaultdict(int)\n",
        "    all_detections = defaultdict(list)\n",
        "\n",
        "    actual_test_dataset = test_dataset.dataset if isinstance(test_dataset, Subset) else test_dataset\n",
        "    is_coco_dataset = isinstance(actual_test_dataset, COCODataset)\n",
        "\n",
        "    log_report(f\"Type de dataset d'évaluation détecté : {'MS COCO' if is_coco_dataset else 'PASCAL VOC'}\", level=\"INFO\")\n",
        "\n",
        "    data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    for image_idx, (image_pil, annotations_or_target, img_path) in enumerate(data_loader):\n",
        "        current_image_gt_boxes = defaultdict(list)\n",
        "\n",
        "        if is_coco_dataset:\n",
        "            coco_target = annotations_or_target\n",
        "            if coco_target['boxes'].numel() > 0:\n",
        "                for bbox_tensor, label_tensor in zip(coco_target['boxes'], coco_target['labels']):\n",
        "                    class_name = PASCAL_VOC_CLASSES[label_tensor.item()]\n",
        "                    bbox = bbox_tensor.tolist()\n",
        "                    current_image_gt_boxes[class_name].append(bbox)\n",
        "                    all_gt_counts_by_class[class_name] += 1\n",
        "        else: # Pascal VOC Dataset\n",
        "            pascal_voc_annotations = annotations_or_target\n",
        "            for gt_obj in pascal_voc_annotations:\n",
        "                class_name = gt_obj['class_name']\n",
        "                bbox = gt_obj['bbox']\n",
        "                current_image_gt_boxes[class_name].append(bbox)\n",
        "                all_gt_counts_by_class[class_name] += 1\n",
        "\n",
        "        detected_objects_in_image = detect_objects(img_path, fine_tuned_cnn_model, trained_svms, iou_nms_threshold, image_transform)\n",
        "\n",
        "        detections_for_image_by_class = defaultdict(list)\n",
        "\n",
        "        for det_class_name, det_score, det_bbox in detected_objects_in_image:\n",
        "            detections_for_image_by_class[det_class_name].append({\n",
        "                'bbox': det_bbox,\n",
        "                'score': det_score,\n",
        "                'tp': False,\n",
        "                'fp': False\n",
        "            })\n",
        "\n",
        "        all_relevant_classes = set(current_image_gt_boxes.keys()).union(set(detections_for_image_by_class.keys()))\n",
        "\n",
        "        for class_name in all_relevant_classes:\n",
        "            image_detections = detections_for_image_by_class[class_name]\n",
        "            image_gts = current_image_gt_boxes[class_name]\n",
        "            image_gts_with_flags = [{'bbox': bbox, 'detected': False} for bbox in image_gts]\n",
        "\n",
        "            image_detections_sorted = sorted(image_detections, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "            for det in image_detections_sorted:\n",
        "                best_iou = 0.0\n",
        "                best_gt_idx = -1\n",
        "\n",
        "                for gt_idx, gt_info in enumerate(image_gts_with_flags):\n",
        "                    iou = calculate_iou(det['bbox'], gt_info['bbox'])\n",
        "                    if iou > best_iou:\n",
        "                        best_iou = iou\n",
        "                        best_gt_idx = gt_idx\n",
        "\n",
        "                if best_iou >= iou_threshold_for_tp and best_gt_idx != -1 and not image_gts_with_flags[best_gt_idx]['detected']:\n",
        "                    det['tp'] = True\n",
        "                    image_gts_with_flags[best_gt_idx]['detected'] = True\n",
        "                else:\n",
        "                    det['fp'] = True\n",
        "\n",
        "                all_detections[class_name].append((det['score'], det['tp']))\n",
        "\n",
        "    mean_ap = 0.0\n",
        "    num_classes_with_gt = 0\n",
        "    ap_per_class = {}\n",
        "\n",
        "    for class_name in PASCAL_VOC_CLASSES:\n",
        "        total_gt_for_class = all_gt_counts_by_class[class_name]\n",
        "\n",
        "        if total_gt_for_class == 0:\n",
        "            ap_per_class[class_name] = 0.0\n",
        "            continue\n",
        "\n",
        "        class_detections = all_detections[class_name]\n",
        "        if not class_detections:\n",
        "            ap_per_class[class_name] = 0.0\n",
        "            continue\n",
        "\n",
        "        class_detections_sorted = sorted(class_detections, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        true_positives = 0\n",
        "        false_positives = 0\n",
        "        precisions = []\n",
        "        recalls = []\n",
        "\n",
        "        for score, is_tp in class_detections_sorted:\n",
        "            if is_tp:\n",
        "                true_positives += 1\n",
        "            else:\n",
        "                false_positives += 1\n",
        "\n",
        "            precision = true_positives / (true_positives + false_positives)\n",
        "            recall = true_positives / total_gt_for_class\n",
        "\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "\n",
        "        ap = 0.0\n",
        "        for t in np.arange(0.0, 1.1, 0.1):\n",
        "            if not precisions:\n",
        "                max_p = 0.0\n",
        "            else:\n",
        "                max_p = 0.0\n",
        "                for i in range(len(recalls)):\n",
        "                    if recalls[i] >= t:\n",
        "                        max_p = max(max_p, precisions[i])\n",
        "            ap += max_p\n",
        "        ap /= 11.0\n",
        "\n",
        "        ap_per_class[class_name] = ap\n",
        "        mean_ap += ap\n",
        "        num_classes_with_gt += 1\n",
        "\n",
        "    if num_classes_with_gt > 0:\n",
        "        mean_ap /= num_classes_with_gt\n",
        "    else:\n",
        "        mean_ap = 0.0\n",
        "\n",
        "    log_report(f\"mAP (IOU={iou_threshold_for_tp}) : {mean_ap:.4f}\", level=\"RESULT\")\n",
        "    report_content_list.append(f\"* mAP (IOU={iou_threshold_for_tp}) : `{mean_ap:.4f}`\\n\")\n",
        "    log_report(\"AP par classe :\", level=\"RESULT\")\n",
        "    report_content_list.append(\"\\n### AP par Classe\\n\")\n",
        "    for class_name, ap_value in ap_per_class.items():\n",
        "        log_report(f\"  - {class_name}: {ap_value:.4f}\", level=\"RESULT\")\n",
        "        report_content_list.append(f\"* {class_name}: `{ap_value:.4f}`\\n\")\n",
        "\n",
        "    save_report_to_file()\n",
        "    return mean_ap, ap_per_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El6plI0LYSno",
        "outputId": "0ba39e7a-1886-4a66-87aa-ced20afddcba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initialisation du pipeline R-CNN ---\n",
            "Heure de début de l'exécution : 2025-06-15 22:13:53\n",
            "[SETUP] [22:13:53] Tentative de montage de Google Drive...\n",
            "Mounted at /content/drive\n",
            "[SETUP] [22:14:18] Google Drive monté avec succès. Les rapports seront enregistrés dans : /content/drive/MyDrive/RCP209/Rapports_RCNN\n",
            "[INFO] [22:14:18] \n",
            "--- Vérification de l'environnement ---\n",
            "[INFO] [22:14:18] PyTorch version: 2.6.0+cu124\n",
            "[INFO] [22:14:18] Torchvision version: 0.21.0+cu124\n",
            "[INFO] [22:14:18] OpenCV version: 4.11.0\n",
            "[INFO] [22:14:18] NumPy version: 2.0.2\n",
            "[INFO] [22:14:18] Scikit-learn version: 1.6.1\n",
            "[INFO] [22:14:18] Un GPU est disponible : NVIDIA A100-SXM4-40GB avec 39.56 GB de mémoire.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_report(\"\\n--- Étape 1 : Préparation de l'environnement et Téléchargement/Vérification du Dataset ---\", to_console=True)\n",
        "\n",
        "TARGET_VOC_ROOT = '/content/VOC2007_dataset'\n",
        "PASCAL_VOC_ROOT = os.path.join(TARGET_VOC_ROOT, 'VOC2007')\n",
        "DOWNLOAD_TEMP_ROOT = '/content/temp_voc_download'\n",
        "\n",
        "VOC07_TRAINVAL_URL = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\"\n",
        "VOC07_TEST_URL = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\"\n",
        "\n",
        "def get_root_dir(output_dir=None, new_folder_name=None):\n",
        "    if output_dir and not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "    if not output_dir:\n",
        "        root_dir = \"./\"\n",
        "    elif os.path.isdir(output_dir):\n",
        "        if new_folder_name:\n",
        "            root_dir = os.path.join(output_dir, new_folder_name)\n",
        "            if not os.path.exists(root_dir):\n",
        "                os.mkdir(root_dir)\n",
        "        else:\n",
        "            root_dir = output_dir\n",
        "    else:\n",
        "        raise ValueError(f\"{output_dir} is not a valid directory or could not be created.\")\n",
        "    return root_dir\n",
        "\n",
        "def get_fname_from_path_or_url(path_or_url):\n",
        "    fname_start = path_or_url.rfind(\"/\") + 1\n",
        "    fname = path_or_url[fname_start:]\n",
        "    return fname\n",
        "\n",
        "def download_one_url(url, root_dir, fname=None):\n",
        "    if not fname:\n",
        "        fname = get_fname_from_path_or_url(url)\n",
        "    output_path = os.path.join(root_dir, fname)\n",
        "    log_report(f\"Downloading {fname} to {root_dir}...\", level=\"INFO\")\n",
        "    with requests.get(url, stream=True, timeout=(10, 300)) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(output_path, \"wb\") as f:\n",
        "            total_size = int(r.headers.get('content-length', 0))\n",
        "            block_size = 8192\n",
        "            for chunk in r.iter_content(chunk_size=block_size):\n",
        "                f.write(chunk)\n",
        "    log_report(f\"Downloaded {fname}.\", level=\"INFO\")\n",
        "    return output_path\n",
        "\n",
        "def unpack(archived_file_path, dest_dir):\n",
        "    fname = get_fname_from_path_or_url(archived_file_path)\n",
        "    log_report(f\"Extracting {fname} to {dest_dir}...\", level=\"INFO\")\n",
        "    shutil.unpack_archive(archived_file_path, dest_dir)\n",
        "    log_report(f\"Extraction of {fname} completed.\", level=\"INFO\")\n",
        "\n",
        "def find_voc2007_root(base_path):\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        if 'VOC2007' in dirs:\n",
        "            voc2007_path = os.path.join(root, 'VOC2007')\n",
        "            if (os.path.exists(os.path.join(voc2007_path, 'JPEGImages')) and\n",
        "                os.path.exists(os.path.join(voc2007_path, 'Annotations')) and\n",
        "                os.path.exists(os.path.join(voc2007_path, 'ImageSets'))):\n",
        "                return voc2007_path\n",
        "    return None\n",
        "\n",
        "if not os.path.exists(os.path.join(PASCAL_VOC_ROOT, 'JPEGImages')):\n",
        "    log_report(\"\\nVOC 2007 non trouvé localement au chemin cible. Démarrage du téléchargement et de la préparation...\", level=\"INFO\")\n",
        "    os.makedirs(DOWNLOAD_TEMP_ROOT, exist_ok=True)\n",
        "    os.makedirs(TARGET_VOC_ROOT, exist_ok=True)\n",
        "\n",
        "    log_report(\"\\nPréparation du dataset d'entraînement/validation...\", level=\"INFO\")\n",
        "    trainval_tar_path = download_one_url(VOC07_TRAINVAL_URL, DOWNLOAD_TEMP_ROOT)\n",
        "    temp_extract_trainval_dir = os.path.join(DOWNLOAD_TEMP_ROOT, 'extracted_trainval')\n",
        "    os.makedirs(temp_extract_trainval_dir, exist_ok=True)\n",
        "    unpack(trainval_tar_path, temp_extract_trainval_dir)\n",
        "    source_voc_trainval = find_voc2007_root(temp_extract_trainval_dir)\n",
        "\n",
        "    if source_voc_trainval:\n",
        "        if os.path.exists(PASCAL_VOC_ROOT):\n",
        "            shutil.rmtree(PASCAL_VOC_ROOT)\n",
        "        shutil.move(source_voc_trainval, PASCAL_VOC_ROOT)\n",
        "        log_report(f\"Contenu de '{source_voc_trainval}' déplacé vers '{PASCAL_VOC_ROOT}'.\", level=\"INFO\")\n",
        "    else:\n",
        "        log_report(f\"ATTENTION: Le dossier 'VOC2007' n'a pas été trouvé dans '{temp_extract_trainval_dir}' après décompression de trainval.\", level=\"ERROR\")\n",
        "        sys.exit(\"Structure de l'archive trainval inattendue. Arrêt du script.\")\n",
        "\n",
        "    log_report(\"\\nPréparation du dataset de test...\", level=\"INFO\")\n",
        "    test_tar_path = download_one_url(VOC07_TEST_URL, DOWNLOAD_TEMP_ROOT)\n",
        "    temp_extract_test_dir = os.path.join(DOWNLOAD_TEMP_ROOT, 'extracted_test')\n",
        "    os.makedirs(temp_extract_test_dir, exist_ok=True)\n",
        "    unpack(test_tar_path, temp_extract_test_dir)\n",
        "    source_voc_test = find_voc2007_root(temp_extract_test_dir)\n",
        "\n",
        "    if source_voc_test:\n",
        "        log_report(f\"Dossier 'VOC2007' trouvé pour test à : {source_voc_test}\", level=\"INFO\")\n",
        "        log_report(f\"Fusion des fichiers de test vers '{PASCAL_VOC_ROOT}'.\", level=\"INFO\")\n",
        "        for item in os.listdir(source_voc_test):\n",
        "            src_item_path = os.path.join(source_voc_test, item)\n",
        "            dest_item_path = os.path.join(PASCAL_VOC_ROOT, item)\n",
        "            if os.path.isdir(src_item_path):\n",
        "                shutil.copytree(src_item_path, dest_item_path, dirs_exist_ok=True)\n",
        "            else:\n",
        "                shutil.copy2(src_item_path, dest_item_path)\n",
        "        log_report(f\"Contenu de '{source_voc_test}' fusionné vers '{PASCAL_VOC_ROOT}'.\", level=\"INFO\")\n",
        "\n",
        "    else:\n",
        "        log_report(f\"ATTENTION: Le dossier 'VOC2007' n'a pas été trouvé dans '{temp_extract_test_dir}' après décompression de test.\", level=\"ERROR\")\n",
        "        sys.exit(\"Structure de l'archive test inattendue. Arrêt du script.\")\n",
        "\n",
        "    if os.path.exists(DOWNLOAD_TEMP_ROOT):\n",
        "        shutil.rmtree(DOWNLOAD_TEMP_ROOT)\n",
        "        log_report(f\"Nettoyé le dossier temporaire : {DOWNLOAD_TEMP_ROOT}\", level=\"INFO\")\n",
        "\n",
        "    log_report(f\"VOC 2007 préparé et consolidé dans : {PASCAL_VOC_ROOT}\", level=\"INFO\")\n",
        "else:\n",
        "    log_report(f\"\\nVOC 2007 déjà préparé dans : {PASCAL_VOC_ROOT}. Skip download/extraction.\", level=\"INFO\")\n",
        "\n",
        "report_content_list.append(f\"* Chemin Dataset PASCAL VOC (consolide trainval et test) : `{PASCAL_VOC_ROOT}`\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFVkYsWUYYpC",
        "outputId": "0dad8866-4de8-4f97-dd41-60100f8d56d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] [22:14:18] \n",
            "--- Étape 1 : Préparation de l'environnement et Téléchargement/Vérification du Dataset ---\n",
            "[INFO] [22:14:18] \n",
            "VOC 2007 non trouvé localement au chemin cible. Démarrage du téléchargement et de la préparation...\n",
            "[INFO] [22:14:18] \n",
            "Préparation du dataset d'entraînement/validation...\n",
            "[INFO] [22:14:18] Downloading VOCtrainval_06-Nov-2007.tar to /content/temp_voc_download...\n",
            "[INFO] [22:14:53] Downloaded VOCtrainval_06-Nov-2007.tar.\n",
            "[INFO] [22:14:53] Extracting VOCtrainval_06-Nov-2007.tar to /content/temp_voc_download/extracted_trainval...\n",
            "[INFO] [22:14:55] Extraction of VOCtrainval_06-Nov-2007.tar completed.\n",
            "[INFO] [22:14:55] Contenu de '/content/temp_voc_download/extracted_trainval/VOCdevkit/VOC2007' déplacé vers '/content/VOC2007_dataset/VOC2007'.\n",
            "[INFO] [22:14:55] \n",
            "Préparation du dataset de test...\n",
            "[INFO] [22:14:55] Downloading VOCtest_06-Nov-2007.tar to /content/temp_voc_download...\n",
            "[INFO] [22:15:21] Downloaded VOCtest_06-Nov-2007.tar.\n",
            "[INFO] [22:15:21] Extracting VOCtest_06-Nov-2007.tar to /content/temp_voc_download/extracted_test...\n",
            "[INFO] [22:15:23] Extraction of VOCtest_06-Nov-2007.tar completed.\n",
            "[INFO] [22:15:23] Dossier 'VOC2007' trouvé pour test à : /content/temp_voc_download/extracted_test/VOCdevkit/VOC2007\n",
            "[INFO] [22:15:23] Fusion des fichiers de test vers '/content/VOC2007_dataset/VOC2007'.\n",
            "[INFO] [22:15:24] Contenu de '/content/temp_voc_download/extracted_test/VOCdevkit/VOC2007' fusionné vers '/content/VOC2007_dataset/VOC2007'.\n",
            "[INFO] [22:15:25] Nettoyé le dossier temporaire : /content/temp_voc_download\n",
            "[INFO] [22:15:25] VOC 2007 préparé et consolidé dans : /content/VOC2007_dataset/VOC2007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_report(\"\\n--- Étape 2 : Chargement et Vérification des Datasets ---\", to_console=True)\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = PascalVOCDataset(root_dir=PASCAL_VOC_ROOT, year='2007', image_set='trainval', transform=image_transform)\n",
        "\n",
        "# --- CHOIX DU DATASET DE TEST ---\n",
        "DATASET_TEST_CHOICE = \"VOC\" # <--- Change ici pour \"VOC\" ou \"COCO\"\n",
        "\n",
        "if DATASET_TEST_CHOICE == \"VOC\":\n",
        "    test_dataset = PascalVOCDataset(root_dir=PASCAL_VOC_ROOT, year='2007', image_set='test', transform=image_transform)\n",
        "    log_report(f\"Dataset de test sélectionné : PASCAL VOC 2007 Test (taille initiale : {len(test_dataset)} images).\", level=\"INFO\")\n",
        "    report_content_list.append(f\"* Dataset de test utilisé : `PASCAL VOC 2007 Test`\\n\")\n",
        "    report_content_list.append(f\"* Chemin Dataset Test : `{PASCAL_VOC_ROOT}`\\n\")\n",
        "\n",
        "elif DATASET_TEST_CHOICE == \"COCO\":\n",
        "    # Ces variables viennent de la Cellule 1\n",
        "    test_dataset = COCODataset(root_dir=COCO_IMAGES_VAL_PATH, annotation_file=COCO_ANNOTATIONS_VAL_PATH, transform=image_transform)\n",
        "    log_report(f\"Dataset de test sélectionné : MS COCO 2017 Val (taille initiale : {len(test_dataset)} images).\", level=\"INFO\")\n",
        "    report_content_list.append(f\"* Dataset de test utilisé : `MS COCO 2017 Val`\\n\")\n",
        "    report_content_list.append(f\"* Chemin Images COCO : `{COCO_IMAGES_VAL_PATH}`\\n\")\n",
        "    report_content_list.append(f\"* Fichier d'annotations COCO : `{COCO_ANNOTATIONS_VAL_PATH}`\\n\")\n",
        "\n",
        "else:\n",
        "    log_report(f\"ERREUR: `DATASET_TEST_CHOICE`='{DATASET_TEST_CHOICE}' est invalide. Utilisez 'VOC' ou 'COCO'.\", level=\"CRITICAL\")\n",
        "    sys.exit(\"Choix de dataset de test invalide. Arrêt du script.\")\n",
        "\n",
        "log_report(f\"Taille du train_dataset initial : {len(train_dataset)} images.\", level=\"INFO\")\n",
        "log_report(f\"Taille du test_dataset initial : {len(test_dataset)} images.\", level=\"INFO\")\n",
        "\n",
        "# --- Limitation de la taille des datasets ---\n",
        "NUM_IMAGES_FOR_TRAINING = 2000 #avant 100 puis 200 maintenat 500\n",
        "log_report(f\"Paramètre : `NUM_IMAGES_FOR_TRAINING` = {NUM_IMAGES_FOR_TRAINING}\", level=\"INFO\")\n",
        "\n",
        "if len(train_dataset) > NUM_IMAGES_FOR_TRAINING:\n",
        "    np.random.seed(42)\n",
        "    indices_train = np.random.choice(len(train_dataset), NUM_IMAGES_FOR_TRAINING, replace=False).tolist()\n",
        "    train_dataset_for_use = Subset(train_dataset, indices_train)\n",
        "    log_report(f\"Dataset d'entraînement **limité** à {len(train_dataset_for_use)} images pour cette exécution.\", level=\"INFO\")\n",
        "else:\n",
        "    train_dataset_for_use = train_dataset\n",
        "    log_report(f\"Le dataset d'entraînement (taille {len(train_dataset)}) est déjà <= à la limite {NUM_IMAGES_FOR_TRAINING}. Aucune limitation appliquée.\", level=\"INFO\")\n",
        "\n",
        "report_content_list.append(f\"* Nombre d'images d'entraînement utilisées : `{len(train_dataset_for_use)}`\\n\")\n",
        "\n",
        "NUM_IMAGES_FOR_TESTING = 100 # avant 50\n",
        "log_report(f\"Paramètre : `NUM_IMAGES_FOR_TESTING` = {NUM_IMAGES_FOR_TESTING}\", level=\"INFO\")\n",
        "\n",
        "if len(test_dataset) > NUM_IMAGES_FOR_TESTING:\n",
        "    np.random.seed(42)\n",
        "    indices_test = np.random.choice(len(test_dataset), NUM_IMAGES_FOR_TESTING, replace=False).tolist()\n",
        "    test_dataset_for_use = Subset(test_dataset, indices_test)\n",
        "    log_report(f\"Dataset de test **limité** à {len(test_dataset_for_use)} images pour cette exécution.\", level=\"INFO\")\n",
        "else:\n",
        "    test_dataset_for_use = test_dataset\n",
        "    log_report(f\"Le dataset de test (taille {len(test_dataset)}) est déjà <= à la limite {NUM_IMAGES_FOR_TESTING}. Aucune limitation appliquée.\", level=\"INFO\")\n",
        "\n",
        "report_content_list.append(f\"* Nombre d'images de test utilisées : `{len(test_dataset_for_use)}`\\n\")\n",
        "\n",
        "log_report(\"\\n--- Chargement des datasets terminé. ---\", to_console=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "j6u_NRMEYnM3",
        "outputId": "d8d60895-cc49-415f-8dd4-64bf6e6bf977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'log_report' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1517465355>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Étape 2 : Chargement et Vérification des Datasets ---\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_console\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m image_transform = transforms.Compose([\n\u001b[1;32m      4\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'log_report' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_report(\"\\n--- Étape 3 : Chargement du CNN (AlexNet) pré-entraîné sur ImageNet ---\", to_console=True)\n",
        "cnn_model = torchvision.models.alexnet(pretrained=True)\n",
        "log_report(\"Modèle AlexNet pré-entraîné chargé. Il est actuellement sur le CPU.\", level=\"INFO\")\n",
        "report_content_list.append(f\"* Modèle CNN : `AlexNet (Pré-entraîné ImageNet)`\\n\")\n",
        "\n",
        "log_report(\"\\n--- Étape 4 : Fine-tuning du CNN (Cette étape utilise le GPU si activé) ---\", to_console=True)\n",
        "fine_tuning_start_time = time.time()\n",
        "\n",
        "# Passez image_transform à fine_tune_cnn\n",
        "fine_tuned_cnn_model = fine_tune_cnn(cnn_model, train_dataset_for_use, num_epochs=5, lr=0.001, image_transform=image_transform) # avant epoch=5 on a fait 10 on revient à 5\n",
        "\n",
        "fine_tuning_end_time = time.time()\n",
        "fine_tuning_duration = fine_tuning_end_time - fine_tuning_start_time\n",
        "log_report(f\"Fine-tuning du CNN terminé en {fine_tuning_duration:.2f} secondes.\", level=\"INFO\")\n",
        "report_content_list.append(f\"* Durée du Fine-tuning CNN : `{fine_tuning_duration:.2f} secondes`\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1SDJVHTYutS",
        "outputId": "e5c436ca-9248-4b62-8fb1-c22ed25c9fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] [22:15:26] \n",
            "--- Étape 3 : Chargement du CNN (AlexNet) pré-entraîné sur ImageNet ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 225MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] [22:15:30] Modèle AlexNet pré-entraîné chargé. Il est actuellement sur le CPU.\n",
            "[INFO] [22:15:30] \n",
            "--- Étape 4 : Fine-tuning du CNN (Cette étape utilise le GPU si activé) ---\n",
            "[INFO] [22:15:30] Démarrage du fine-tuning du CNN pour 5 époques...\n",
            "[INFO] [23:21:42] Epoch 1/5, Loss: 0.6098, Accuracy: 0.8372\n",
            "[INFO] [00:27:37] Epoch 2/5, Loss: 0.4108, Accuracy: 0.8789\n",
            "[INFO] [01:33:33] Epoch 3/5, Loss: 0.3311, Accuracy: 0.8994\n",
            "[INFO] [02:39:40] Epoch 4/5, Loss: 0.2783, Accuracy: 0.9132\n",
            "[INFO] [03:45:51] Epoch 5/5, Loss: 0.2176, Accuracy: 0.9306\n",
            "[INFO] [03:45:51] Fine-tuning du CNN terminé.\n",
            "[INFO] [03:45:51] Fine-tuning du CNN terminé en 19820.91 secondes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_report(\"\\n--- Étape 5 : Entraînement des SVMs (Cette étape utilise le GPU si activé pour l'extraction de features) ---\", to_console=True)\n",
        "svms_training_start_time = time.time()\n",
        "\n",
        "# Passez image_transform à train_svms\n",
        "trained_svms = train_svms(train_dataset_for_use, fine_tuned_cnn_model, image_transform=image_transform)\n",
        "\n",
        "svms_training_end_time = time.time()\n",
        "svms_training_duration = svms_training_end_time - svms_training_start_time\n",
        "log_report(f\"Entraînement des SVMs terminé en {svms_training_duration:.2f} secondes.\", level=\"INFO\")\n",
        "report_content_list.append(f\"* Durée de l'entraînement des SVMs : `{svms_training_duration:.2f} secondes`\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "P3MHoi33ZCc1",
        "outputId": "a3d98d2c-1cd7-496d-8857-1113981ee011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'log_report' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3068146689>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Étape 5 : Entraînement des SVMs (Cette étape utilise le GPU si activé pour l'extraction de features) ---\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_console\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msvms_training_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Passez image_transform à train_svms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrained_svms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_svms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_for_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tuned_cnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'log_report' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_report(\"\\n--- Étape 6 : Démarrage de l'évaluation du détecteur (Cette étape utilise le GPU si activé) ---\", to_console=True)\n",
        "evaluation_start_time = time.time()\n",
        "\n",
        "# Passez image_transform à evaluate_detector\n",
        "mean_ap, ap_per_class_results = evaluate_detector(\n",
        "    test_dataset_for_use,\n",
        "    fine_tuned_cnn_model,\n",
        "    trained_svms,\n",
        "    image_transform=image_transform\n",
        ")\n",
        "\n",
        "evaluation_end_time = time.time()\n",
        "evaluation_duration = evaluation_end_time - evaluation_start_time\n",
        "log_report(f\"Évaluation du détecteur terminée en {evaluation_duration:.2f} secondes.\", level=\"INFO\")\n",
        "report_content_list.append(f\"* Durée de l'évaluation du détecteur : `{evaluation_duration:.2f} secondes`\\n\")\n",
        "\n",
        "# --- Fin de l'exécution globale ---\n",
        "end_time_global = datetime.now()\n",
        "total_duration_global = end_time_global - start_time_global\n",
        "log_report(f\"\\n--- Exécution complète du pipeline R-CNN terminée ---\", to_console=True)\n",
        "log_report(f\"Heure de fin : {end_time_global.strftime('%Y-%m-%d %H:%M:%S')}\", to_console=True)\n",
        "log_report(f\"Durée totale de l'exécution : {total_duration_global}\", to_console=True)\n",
        "\n",
        "report_content_list.append(f\"\\n---\\n\\n## Résultat Final\\n\")\n",
        "report_content_list.append(f\"* Heure de fin de l'exécution : `{end_time_global.strftime('%Y-%m-%d %H:%M:%S')}`\\n\")\n",
        "report_content_list.append(f\"* Durée totale de l'exécution : `{total_duration_global}`\\n\")\n",
        "\n",
        "save_report_to_file()\n",
        "print(\"\\nProcessus terminé.\")"
      ],
      "metadata": {
        "id": "Ao7B6SR_ZE31"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}